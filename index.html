<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Will LLMs Scaling Hit the Wall? Breaking Barriers with Distributed Resources on Massive Edge Devices">
  <meta property="og:title" content="Will LLMs Scaling Hit the Wall?"/>
  <meta property="og:description" content="Breaking Barriers with Distributed Resources on Massive Edge Devices"/>
  <meta property="og:url" content="https://tao.shen.zju.edu.cn/distributed-llm-edges/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/fedllm.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Will LLMs Scaling Hit the Wall?">
  <meta name="twitter:description" content="Breaking Barriers with Distributed Resources on Massive Edge Devices">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/fedllm.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="LLM, Edge Devices, Distributed Computing, Federated Learning, AI Democratization">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Will LLMs Scaling Hit the Wall?</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css" media="print" onload="this.media='all'">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Preload critical visualization resources -->
  <link rel="preload" href="htmls/iot_data_contribution_interactive.html" as="document">
  <link rel="preload" href="htmls/compute_trend_interactive.html" as="document">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js" defer></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js" defer></script>
  <script src="static/js/bulma-slider.min.js" defer></script>
  <script src="static/js/index.js" defer></script>
  
  <script>
    // Optimized iframe loading strategy
    document.addEventListener('DOMContentLoaded', function() {
      const iframes = document.querySelectorAll('iframe[loading="lazy"]');
      
      // Store original src to data attribute and remove from iframe
      iframes.forEach(iframe => {
        iframe.setAttribute('data-src', iframe.src);
        iframe.removeAttribute('src');
        iframe.classList.add('lazy-iframe');
      });
      
      // Function to load iframes when they come into view
      const loadIframes = () => {
        const lazyIframes = document.querySelectorAll('.lazy-iframe');
        
        lazyIframes.forEach(iframe => {
          const rect = iframe.getBoundingClientRect();
          // Load iframe when it's within 800px of the viewport
          if (rect.top <= window.innerHeight + 800 && rect.bottom >= -800) {
            iframe.src = iframe.getAttribute('data-src');
            iframe.classList.remove('lazy-iframe');
          }
        });
      };
      
      // Check for iframes on page load and scroll
      loadIframes();
      window.addEventListener('scroll', loadIframes);
      window.addEventListener('resize', loadIframes);
    });
  </script>
</head>
<body>


  <style>
    .hero {
      position: relative;
      overflow: hidden;
      background-color: white;
    }

    .hero-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      opacity: 0.3;
      z-index: 0;
      pointer-events: none;
    }

    .hero-background iframe {
      width: 100%;
      height: 100%;
      transform: scale(1.42) translateY(-15%);
      transform-origin: center center;
    }

    .hero-body {
      position: relative;
      z-index: 1;
    }
  </style>

  <section class="hero">
    <div class="hero-background">
      <iframe src="htmls/fedllm.html" frameborder="0" scrolling="no"></iframe>
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Will LLMs Scaling Hit the Wall? Breaking Barriers with Distributed Resources on Massive Edge Devices</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=MeaDj20AAAAJ" target="_blank">Tao Shen</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://didizhu-zju.github.io" target="_blank">Didi Zhu</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=GzZxXIcAAAAJ" target="_blank">Ziyu Zhao</a><sup>*</sup>,</span>
                    <span class="author-block">
                      <a href="https://scholar.google.com/citations?user=gpTPt58AAAAJ" target="_blank">Chao Wu</a>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=XJLn4MYAAAAJ" target="_blank">Fei Wu</a>
                      </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Zhejiang University</span>
                    <!-- <span class="author-block">Zhejiang University<br>ICML 2025 (Position Paper)</span> -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="icml25_position/example_paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Github link -->
                    <span class="link-block">
                      <a href="https://github.com/tao-shen/Distributed-LLM-Edges" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The success of foundation models relies on scaling laws, which show that model performance improves predictably with increased <em>training data</em> and <em>model size</em>. 
              However, this scaling trajectory faces two critical challenges: depletion of high-quality public data and monopolization of computational power required for larger models by tech giants. These bottlenecks severely hinder AI advancement.
            <!-- </p>
            <p> -->
              In this position paper, we advocate leveraging massive distributed edge devices to break through these barriers.
              We reveal the vast untapped potential of data and computational resources on edge devices, and review recent technical advancements in distributed/federated learning that make this paradigm viable.
              Our analysis indicates that through edge device collaboration, anyone can participate in training large language models using small edge devices.
              This shift toward distributed edge training can democratize AI development and foster a more inclusive AI community.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Research Insights Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- The Challenge -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">The Challenge: Hitting the Wall</h2>
          <div class="content has-text-justified">
            <p>
              <strong>Data Depletion:</strong> Neural scaling laws establish that model performance improves predictably with data quantity, requiring exponentially growing datasets that expand by approximately 0.38 orders of magnitude (2.4×) annually. Despite the internet's vast resources, high-quality human-generated text remains bounded at approximately 4×10<sup>14</sup> tokens, with researchers predicting <span class="highlight-text">exhaustion of public text data by 2028</span> (potentially accelerated to 2026 through excessive reuse). 
            </p>
            
            <p>
              <strong>Computational Monopoly:</strong> The AI computing landscape is dominated by a few tech giants (OpenAI, Google, Microsoft, Meta) controlling advanced hardware resources. This monopolization creates significant barriers for smaller organizations. Since 2010, AI training demands have grown at a super-exponential rate of 3.9× per year, further accelerating to <span class="highlight-text">13.4× per year since 2022</span> with large language models (See Figure 1). This challenge is compounded by Moore's Law slowing as we approach silicon-based chip technology limits, while infrastructure capacity faces dual constraints from semiconductor manufacturing bottlenecks and limited production capacity against exponentially growing deployment needs.
            </p>
            
            <!-- Computing Trend Visualization -->
            <div class="visualization-container">
              <iframe src="htmls/compute_trend_interactive.html" width="100%" height="550px" frameborder="0" scrolling="no" class="visualization-iframe" loading="lazy"></iframe>
              <p class="has-text-centered">
                <em>Figure 1: AI computational demands have grown exponentially, reaching an unprecedented 13.4× annual growth rate since 2022, pushing against physical and economic limits.</em>
              </p>
            </div>
            
            <!-- <p>
              The computing challenge is compounded by Moore's Law slowing down as we approach the physical limits of silicon-based chip technology. The difficulty in shrinking transistors has led to diminishing returns in computational performance. Additionally, infrastructure capacity faces dual constraints: bottlenecks in advanced semiconductor manufacturing severely limit the expansion rate of AI data centers, while the exponential increase in chip deployment pushes against limited semiconductor production capacity.
            </p> -->
          </div>
        </div>
      </div>

      <!-- The Opportunity -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">The Opportunity: Edge Resources</h2>
          <div class="content has-text-justified">
            <p>
              <strong>Untapped Edge Data:</strong> Edge devices offer a crucial solution to data exhaustion challenges. By 2025, global data volume is projected to reach 182 ZB, with IoT devices contributing significantly—increasing <span class="highlight-text">from 13.6 ZB in 2019 to 79.4 ZB in 2025</span> (See Figure 2). Smartphone data volume specifically is forecast to grow from 5 EB in 2018 to 8 EB by 2028, with the accumulated smartphone data of the past 5 years (before 2025) estimated at approximately 33.1 EB (See Figure 3).
            </p>
            
            <!-- Data Growth Visualization -->
            <div class="visualization-container">
              <iframe src="htmls/iot_data_contribution_interactive.html" width="100%" height="520px" frameborder="0" scrolling="no" class="visualization-iframe" loading="lazy"></iframe>
              <p class="has-text-centered">
                <em>Figure 2: Global data volume is growing exponentially, with IoT devices contributing significantly to this growth (from 33.2% to 43.6% over 2015-2025).</em>
              </p>
            </div>

            <!-- Edge Computing and Smartphone Visualization -->
            <div class="visualization-container">
              <iframe src="htmls/edge_and_smartphone.html" width="100%" height="520px" frameborder="0" scrolling="no" class="visualization-iframe" loading="lazy"></iframe>
              <p class="has-text-centered">
                <em>Figure 3: Smartphone data volume is projected to grow from 5 EB in 2018 to 8 EB by 2028, while the edge computing market is forecasted to surge from $5.5 billion to $87.9 billion in the same period.</em>
              </p>
            </div>
            
            <div class="content has-text-justified highlight-box">
              <p>
                <strong>Key Insight:</strong> The smartphone data volume of the past 5 years (before 2025) is estimated to reach approximately <span class="highlight-text">33.1 EB</span>, offering unique advantages in privacy and real-time environments, demonstrating the enormous potential of edge devices for AI training.
              </p>
            </div>
            
            <!-- <p>
              Edge data provides several qualitative advantages for model training: superior <strong>diversity</strong> across domains, languages, and user behaviors; strong <strong>real-time capability</strong> with continuously fresh data; enhanced <strong>personalization</strong> for context-aware adaptations; and improved <strong>contextual awareness</strong> due to proximity of devices to data sources.
            </p> -->
            
            <p>
              <strong>Collective Computing Power:</strong> Computing capabilities across edge devices have shown sustained growth, with desktop, laptop, and mobile devices demonstrating annual growth rates of 1.29×, 1.20×, and 1.20× respectively (See Figure 4). This growth trajectory presents a stark contrast to the unsustainable computational demands of centralized AI training. In the smartphone sector specifically, the collective computing power has increased dramatically from <span class="highlight-text">817 EFLOPS in 2020 to 2,758 EFLOPS in 2024</span>, with Samsung (27.5%), Xiaomi (25.5%), and Apple (17.2%) contributing significantly to this growth through their advanced mobile processors (See Figure 5).
            </p>

            <!-- Yearly Mean Compute Trend -->
            <div class="visualization-container">
              <iframe src="htmls/compute_trend_yearly_mean.html" width="100%" height="520px" frameborder="0" scrolling="no" class="visualization-iframe" loading="lazy"></iframe>
              <p class="has-text-centered">
                <em>Figure 4: The yearly growth in compute requirements shows the unsustainable trajectory of centralized AI training, highlighting the need for more efficient and distributed approaches to advance AI capabilities.</em>
              </p>
            </div>
            
            <!-- Smartphone Compute Trend -->
            <div class="visualization-container">
              <iframe src="htmls/smartphone_compute_trend.html" width="100%" height="520px" frameborder="0" scrolling="no" class="visualization-iframe" loading="lazy"></iframe>
              <p class="has-text-centered">
                <em>Figure 5: The collective computing capabilities of smartphones have grown dramatically from 817 EFLOPS in 2020 to 2,758 EFLOPS in 2024, providing a formidable distributed training resource.</em>
              </p>
            </div>
            
            <div class="content has-text-justified highlight-box">
              <p>
                <strong>Key Insight:</strong> The cumulative smartphone computing power over the past 5 years totals <span class="highlight-text">9,278 EFLOPS</span>. Modern flagship devices achieve over 2 TFLOPS per unit, with remarkable efficiency—just 30 current-generation smartphones working in parallel can match the computational capacity of an H100 GPU (59.30 TFLOPS), demonstrating the potential for distributed AI training.
              </p>
            </div>
            
            <p>
              This vast distributed computing resource presents a practical path to democratizing AI development: training a state-of-the-art model comparable to DeepSeek-v3 would require approximately <span class="highlight-text">60,723 edge devices working in parallel for one week</span>. Given the <em>billions of smartphones in use globally</em>, this represents a highly feasible approach to distributed AI training, potentially breaking the computational monopoly of large tech companies.
            </p>
          </div>
        </div>
      </div>

      <!-- Technical Approaches -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Technical Approaches</h2>
          <div class="content has-text-justified">
            <p>
              <strong>Small Language Models at Edges:</strong> Deploying compact language models on edge devices significantly reduces computational and memory requirements while maintaining acceptable performance. Recent advancements in model compression, knowledge distillation, and quantization have enabled increasingly powerful yet efficient models on resource-constrained devices.
            </p>
            
            <p>
              <strong>Collaborative Inference:</strong> Distributing inference across multiple devices allows more complex models to run without requiring any single device to handle the entire computational load. This approach enables larger models than possible on individual devices while maintaining low latency and reducing bandwidth requirements.
            </p>
            
            <p>
              <strong>Collaborative Training:</strong> Federated learning enables model training across distributed devices without requiring data to leave the device, preserving privacy while leveraging collective computational power. Recent initiatives like Prime Intellect's INTELLECT-1 project (10B parameter model) utilize the OpenDiLoCo framework to significantly reduce inter-node communication costs, achieving 83% compute utilization across 112 H100 GPUs in five countries. Similarly, Flower Lab's FlowerLLM has successfully trained a 1.3B parameter model using novel federated learning methods.
            </p>
            
            <!-- Federated LLM Visualization -->
            <div class="visualization-container">
              <iframe src="htmls/fedllm.html" width="100%" height="580px" frameborder="0" scrolling="no" class="visualization-iframe fedllm-viz" loading="lazy"></iframe>
              <p class="has-text-centered">
                <em>Figure 5: Federated learning enables distributed training across edge devices while preserving privacy, allowing for collaborative model development without compromising sensitive data.</em>
              </p>
            </div>
            
            <p>
              A key limitation of traditional federated learning is its requirement for each participant to maintain a complete model locally—problematic for large language models. Novel approaches are being developed to address this constraint, enabling distributed training across devices with varying computational capabilities, showing promise for large-scale collaborative training in privacy-sensitive domains with unevenly distributed computational resources.
            </p>
          </div>
        </div>
      </div>

      <!-- Societal Impact -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Societal Impact</h2>
          <div class="content has-text-justified">
            <p>
              <strong>AI Democratization:</strong> By leveraging edge resources, we can create a more inclusive environment where diverse participants collectively develop and train powerful language models. This reduces dependence on major tech companies and promotes a more diverse AI research and applications ecosystem.
            </p>
            
            <div class="highlight-box">
              <p>
                <strong>Key Insight:</strong> Distributed edge training significantly lowers barriers to AI development participation, enabling smaller organizations, academic institutions, and individual developers to contribute to large language model training and innovation.
              </p>
            </div>
            
            <p>
              <strong>Privacy and Data Ownership:</strong> Federated learning allows data to remain on user devices, reducing privacy risks and giving users greater control over their data—increasingly important with stringent global privacy regulations.
            </p>
            
            <p>
              <strong>Environmental Sustainability:</strong> Edge computing utilizes idle computing capacity of existing devices, reducing energy consumption and the need for dedicated data centers. Leveraging billions of devices already in operation lowers the carbon footprint associated with AI training infrastructure.
            </p>
          </div>
        </div>
      </div>

      <!-- Conclusion -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Conclusion and Outlook</h2>
          <div class="content has-text-justified">
            <p>
              Distributed edge devices represent a vast untapped resource that can drive the next generation of large language model development. By leveraging these resources, we can overcome data depletion and computational monopoly challenges while enabling more efficient, private, and inclusive AI advancement.
            </p>
            
            <p>
              Looking ahead, we anticipate:
            </p>
            
            <ul>
              <li>Continuous enhancement of edge device hardware capabilities</li>
              <li>More efficient distributed learning algorithms that minimize communication overhead</li>
              <li>Specialized small language model architectures optimized for edge deployment</li>
              <li>Advanced frameworks supporting secure, privacy-preserving collaborative learning</li>
            </ul>
            
            <div class="highlight-box">
              <p>
                <strong>Key Outlook:</strong> The distributed capacity of edge devices will foster a democratized AI ecosystem where developers worldwide can participate in training and applying large language models, addressing broader societal needs and unlocking new possibilities for AI innovation.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Research Insights Section -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{shen2025will,
  title={Will LLMs Scaling Hit the Wall? Breaking Barriers with Distributed Resources on Massive Edge Devices},
  author={Tao Shen and Didi Zhu and Ziyu Zhao and Chao Wu and Fei Wu},
  booktitle={Arxiv},
  year={2025}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Add some custom CSS for visualization containers -->
  <style>
    .visualization-container {
      margin: 2rem 0;
      padding: 1rem;
      background-color: #f9f9f9;
      border-radius: 8px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
      overflow: hidden; /* Prevents any potential overflow */
      position: relative;
    }
    
    .visualization-container iframe {
      display: block;
      margin: 0 auto;
      max-width: none; /* 移除最大宽度限制 */
      border: none;
      transform: scale(0.75);
      transform-origin: 0 0;
      width: 200% !important; /* 进一步增加宽度，1/0.75 = 133% */
      height: 800px !important; /* 减小高度以减少空白空间 */
      margin-bottom: -180px; /* 负margin使caption上移 */
      will-change: transform; /* 优化渲染性能 */
    }

    /* 联邦学习图表专用样式 */
    .fedllm-viz {
      transform: scale(0.61) !important;
      width: 300% !important; /* 1/0.7 = 143% */
      height: 700px !important;
      margin-bottom: -300px !important;
      margin-top: -50px !important;
    }
    
    .visualization-container p {
      margin-top: 0; /* 移除caption顶部margin */
      position: relative; /* 相对定位以便与iframe重叠 */
      font-size: 0.9rem;
      z-index: 2; /* 确保caption在iframe之上 */
    }
    
    /* 关键发现突出框样式 */
    .highlight-box {
      margin: 1.5rem 0;
      padding: 1rem 1.5rem;
      background-color: #ECF6FF;
      border-left: 4px solid #0164E0;
      border-radius: 0 8px 8px 0;
      box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }
    
    .highlight-box p {
      margin: 0;
      font-size: 1rem;
      line-height: 1.5;
    }
    
    .highlight-box strong {
      color: #0164E0;
    }
    
    /* Lazy loading iframe placeholder */
    .lazy-iframe {
      background: #f5f5f5 url('data:image/svg+xml;charset=utf-8,%3Csvg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"%3E%3Cpath fill="%23ccc" d="M50 25a3 3 0 100 50 3 3 0 000-50z"%3E%3CanimateTransform attributeName="opacity" from="1" to=".3" dur="0.8s" repeatCount="indefinite" /%3E%3C/path%3E%3C/svg%3E') no-repeat center center;
      background-size: 50px;
    }
    
    /* Responsive adjustments */
    @media screen and (max-width: 768px) {
      .visualization-iframe {
        transform: scale(0.65);
        width: 154% !important; /* 1/0.65 = 154% */
        height: 750px !important;
        margin-bottom: -80px; /* 为平板设备调整 */
      }
    }
    
    @media screen and (max-width: 480px) {
      .visualization-iframe {
        transform: scale(0.55);
        width: 182% !important; /* 1/0.55 = 182% */
        height: 850px !important;
        margin-bottom: -60px; /* 为手机设备调整 */
      }
    }

    .highlight-text {
      color: #0164E0;
      font-weight: 500;
      border-bottom: 1px dotted #0164E0;
    }
    .emphasis-text {
      color: #0164E0;
    }
    .growth-rate {
      font-family: "Courier New", monospace;
      background-color: #f8f9fa;
      padding: 0 4px;
      border-radius: 3px;
    }
    mark {
      background-color: transparent;
      border-bottom: 2px solid #0164E0;
      padding: 0 1px;
    }
  </style>

</body>
</html>
